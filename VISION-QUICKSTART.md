# Vision System Quick Start

Get started with the vision-first architecture in 5 minutes.

## What is This?

Nanobrowser now includes a vision-first system that:
- **Sees** web pages like a human (using screenshots)
- **Understands** layouts using Qwen3-VL-30B AI
- **Learns** UI element locations for instant future actions
- **Acts** with pixel-perfect coordinate-based interactions

## Prerequisites

1. **API Key** from one of:
   - [HuggingFace](https://huggingface.co/settings/tokens) (recommended)
   - [DashScope](https://dashscope.console.aliyun.com/) (AliCloud)
   - vLLM self-hosted (no key needed)
   - Custom OpenAI-compatible endpoint

2. **Nanobrowser Extension** installed

## 5-Minute Setup

### Step 1: Get an API Key

**Option A: HuggingFace (Easiest)**
1. Go to https://huggingface.co/settings/tokens
2. Click "New token"
3. Select "Read" permissions
4. Copy the token (starts with `hf_...`)

**Option B: DashScope (AliCloud)**
1. Go to https://dashscope.console.aliyun.com/
2. Create account and get API key

**Option C: Self-Hosted vLLM**
1. Run: `vllm serve Qwen/Qwen2-VL-72B-Instruct`
2. No API key needed

### Step 2: Configure Nanobrowser

*(Once the UI integration is complete)*

1. Click Nanobrowser icon â†’ Settings
2. Enable "Vision Mode"
3. Choose provider: HuggingFace
4. Paste API key: `hf_xxxxx`
5. Model: `Qwen/Qwen2-VL-72B-Instruct`
6. Save

### Step 3: Try Teaching Mode

1. Navigate to any website (e.g., google.com)
2. Click "Start Teaching Mode" in Nanobrowser panel
3. System detects all buttons, inputs, links
4. See numbered red boxes appear on page
5. Click an element you want to label
6. Type a name: "search button"
7. Repeat for other important elements
8. Click "Save & Exit"

**That's it!** Elements are now remembered.

### Step 4: Use Learned Elements

Next time you visit the same page:
1. Just say: "Click the search button"
2. Nanobrowser clicks instantly (no vision API call needed!)
3. **Result**: 0ms latency, 0 API cost

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  First Visit: Teaching Mode                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Capture screenshot                                  â”‚
â”‚  2. Send to Qwen3-VL â†’ "Find all buttons"              â”‚
â”‚  3. Receive: <box>(100,50),(250,90)</box> Search       â”‚
â”‚  4. Draw numbered overlays on page                      â”‚
â”‚  5. User labels: "1 = search button"                    â”‚
â”‚  6. Save to chrome.storage.local                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Future Visits: Memory Mode (FAST!)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Load from chrome.storage.local                      â”‚
â”‚  2. Convert coordinates to current viewport             â”‚
â”‚  3. Click directly                                      â”‚
â”‚  âš¡ Total time: <50ms, No API calls!                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Example Tasks

### Task 1: Google Search
```
You: "Search for AI agents on Google"

Vision System:
1. Detect search input box â†’ (500, 200)
2. Type "AI agents" at coordinates
3. Detect search button â†’ (650, 240)
4. Click at coordinates
5. Done!
```

### Task 2: Login Flow
```
You: "Fill login form: user@email.com / password123"

Vision System:
1. Detect email input â†’ (400, 300)
2. Type email at coordinates
3. Detect password input â†’ (400, 350)
4. Type password at coordinates
5. Detect login button â†’ (450, 400)
6. Click at coordinates
7. Done!
```

### Task 3: Shopping
```
You: "Find headphones under $50"

Vision System:
1. Detect search bar
2. Type "headphones"
3. Detect filter menu
4. Click "Price" filter
5. Click "Under $50"
6. Extract results using vision
```

## API Costs

**Teaching Mode**: ~$0.01-0.05 per page (one-time)
**Memory Mode**: $0 (uses stored data)
**Vision Mode**: ~$0.01 per action

**Recommendation**: Teach frequently-used pages, save API costs on repeated visits.

## Key Commands

*(Once integrated)*

- `"Start teaching"` - Enter teaching mode
- `"Annotate element 5 as login button"` - Label element
- `"Click the search button"` - Use learned element
- `"Click at 500, 300"` - Manual coordinate click
- `"End teaching"` - Save and exit

## Debugging

### See What Vision Sees
Enable debug mode to see:
- Detected bounding boxes
- Confidence scores
- Element descriptions
- Coordinate mappings

### Common Issues

**"Element not found"**
- Re-run teaching mode on page
- Check if layout changed
- Try more specific description

**"API key invalid"**
- Verify key is correct
- Check provider setting
- Test key on provider website

**"Clicks wrong location"**
- Viewport size changed
- Re-teach the page
- Check coordinate normalization

## Advanced Features

### Multi-Language Support
Vision works with any language - it sees visually:
- Chinese: "ç‚¹å‡»æœç´¢æŒ‰é’®"
- Japanese: "æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯"
- Arabic: "Ø§Ù†Ù‚Ø± ÙÙˆÙ‚ Ø²Ø± Ø§Ù„Ø¨Ø­Ø«"

### Responsive Design
Coordinates auto-adjust to:
- Window resizes
- Zoom levels
- Different screen sizes
- Mobile vs desktop

### Privacy
- Screenshots never leave your machine (unless sent to API)
- Use vLLM for 100% local processing
- Clear stored memories anytime
- No tracking or telemetry

## Performance Tips

1. **Teach Once**: Teach frequently-used pages to avoid repeat API calls
2. **Batch Actions**: Multiple actions on same screenshot = 1 API call
3. **Use Memory Mode**: 100x faster than vision mode
4. **Local Models**: vLLM for zero API cost and max privacy

## Next Steps

1. âœ… Read [VISION-SYSTEM.md](./VISION-SYSTEM.md) for complete API docs
2. âœ… Read [VISION-INTEGRATION-GUIDE.md](./VISION-INTEGRATION-GUIDE.md) for integration
3. âœ… Check [examples/](./examples/) for code samples
4. ğŸš€ Start automating!

## Support

- **Questions**: Open GitHub issue
- **Bug Reports**: Use GitHub issues
- **Feature Requests**: GitHub discussions
- **Community**: Join Discord

## Philosophy

> "The best interface is the one you can see and understand"

Traditional DOM-based automation breaks when:
- Classes change (Tailwind CSS updates)
- IDs are randomized
- Shadow DOM blocks access
- Iframes complicate selection

Vision-based automation works because:
- âœ… Sees what users see
- âœ… Adapts to visual changes
- âœ… Works across any website
- âœ… Natural language descriptions
- âœ… Resolution independent

## Comparison

| Method | Speed | Flexibility | Maintenance |
|--------|-------|-------------|-------------|
| DOM Selectors | âš¡ Fast | âš ï¸ Brittle | ğŸ˜ High |
| Accessibility Tree | âš¡ Fast | âš ï¸ Limited | ğŸ˜ Medium |
| **Vision + Memory** | âš¡ **Fast** | âœ… **Universal** | ğŸ˜Š **Low** |
| Vision Only | ğŸŒ Slow | âœ… Universal | ğŸ˜Š Low |

**Winner**: Vision + Memory (best of both worlds)

## Success Stories

*(To be added as users share feedback)*

## Roadmap

- [x] Core vision infrastructure
- [x] Teaching/memory system
- [x] Coordinate-based actions
- [x] Multi-provider support
- [ ] UI integration (in progress)
- [ ] Full-page screenshot stitching
- [ ] CLIP embeddings
- [ ] OCR for text-heavy pages
- [ ] Video/animation handling
- [ ] Mobile device support

---

**Ready to try it?** Follow the setup steps above and start automating! ğŸš€
